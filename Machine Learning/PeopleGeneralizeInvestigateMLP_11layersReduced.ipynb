{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAAIMS: Predicting Multiple Sclerosis from Dynamics of Gait Variability Using an Instrumented Treadmill - A Machine Learning-Based Approach\n",
    "\n",
    "### Person generalization ML models and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rk4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import xgboost \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from inspect import signature\n",
    "from scipy import interp\n",
    "from pyitlib import discrete_random_variable as drv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\rk4\\\\Desktop\\\\GAIT\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape:  (3230, 34)\n",
      "Size-N data shape:  (3230, 34)\n",
      "Regress-N data shape:  (3230, 25)\n",
      "Raw data shape:  (3230, 24)\n",
      "Size-N data shape:  (3230, 24)\n",
      "Regress-N data shape:  (3230, 24)\n"
     ]
    }
   ],
   "source": [
    "#Reading the raw dataframe \n",
    "raw_df = pd.read_csv(path+'FinalCodes\\\\gait_features.csv', index_col = 0)\n",
    "#Dropping the NaNs\n",
    "raw_df.dropna(inplace = True)\n",
    "#Resetting the index\n",
    "raw_df.reset_index(inplace= True)\n",
    "print ('Raw data shape: ', raw_df.shape)\n",
    "\n",
    "#Reading the Size-N dataframe \n",
    "sizeN_df = pd.read_csv(path+'FinalCodes\\\\size_normalized_gait_features.csv', index_col = 0)\n",
    "#Dropping the NaNs\n",
    "sizeN_df.dropna(inplace = True)\n",
    "#Resetting the index\n",
    "sizeN_df.reset_index(inplace= True)\n",
    "print ('Size-N data shape: ', sizeN_df.shape)\n",
    "\n",
    "#Reading the Regress-N dataframe \n",
    "regressN_df = pd.read_csv(path+'FinalCodes\\\\mr_scaled_features_30controlsTrialW.csv', index_col = 0)\n",
    "regressN_df.reset_index(inplace= True)\n",
    "print('Regress-N data shape: ', regressN_df.shape)\n",
    "\n",
    "#Delete the treadmill speeds as features since they are very very correlated with stride speed\n",
    "#Also delete Butterfly plot y-direction features since COP_Y is not adjusted \n",
    "#Swing time and SS_L are the same\n",
    "to_drop = ['tspeed_HSR', 'tspeed_MidSSR', 'tspeed_TOR', 'tspeed_HSL', 'tspeed_TOL', 'tspeed_MidSSL',  'Butterfly_y_abs', \n",
    "           'ButterflySQ_y', 'SS_L', 'index']\n",
    "raw_df.drop(to_drop, axis = 1, inplace= True)\n",
    "raw_df = shuffle(raw_df, random_state = 0)\n",
    "print ('Raw data shape: ', raw_df.shape) #21 features + PID + Trial ID + Label = 24 features \n",
    "\n",
    "sizeN_df.drop(to_drop, axis = 1, inplace= True)\n",
    "sizeN_df = shuffle(sizeN_df, random_state = 0)\n",
    "print ('Size-N data shape: ', sizeN_df.shape) #21 features + PID + Trial ID + Label = 24 features \n",
    "\n",
    "regressN_df.drop(['index'], axis = 1, inplace = True)\n",
    "regressN_df = shuffle(regressN_df, random_state = 0)\n",
    "print('Regress-N data shape: ', regressN_df.shape)  #21 features + PID + Trial ID + Label = 24 features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, yoriginal_, ypredicted_):\n",
    "    best_index = model.cv_results_['mean_test_accuracy'].argmax()\n",
    "    print('best_params: ', model.cv_results_['params'][best_index])\n",
    "\n",
    "    #Stride-wise metrics \n",
    "    stride_metrics_mean, stride_metrics_std = [], [] #Mean and SD of stride based metrics - Acc, P, R, F1, AUC (in order)\n",
    "    scores={'accuracy': make_scorer(acc), 'precision':'precision', 'recall':'recall', 'f1': 'f1', 'auc': 'roc_auc'}\n",
    "    for score in scores:\n",
    "        stride_metrics_mean.append(model.cv_results_['mean_test_'+score][best_index])\n",
    "        stride_metrics_std.append(model.cv_results_['std_test_'+score][best_index])\n",
    "    print('Stride-based model performance (mean): ', stride_metrics_mean)\n",
    "    print('Stride-based model performance (standard deviation): ', stride_metrics_std)\n",
    "    n_folds = 5\n",
    "    person_acc, person_p, person_r, person_f1, person_auc = [], [], [], [], []\n",
    "    #For ROC curves \n",
    "    tpr_list = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        #For each fold, there are 2 splits: test and train (in order) and we need to retrieve the index \n",
    "        #of only test set for required 5 folds (best index)\n",
    "        temp = test_features.loc[yoriginal_[(best_index*10) + (2*i)].index] #True labels for the test strides in each fold\n",
    "        temp['pred'] = ypredicted_[(best_index*10) + (2*i)] #Predicted labels for the strides in the test set in each fold\n",
    "\n",
    "        #Correctly classified strides i.e. 1 if stride is correctly classified and 0 if otherwise\n",
    "        temp['correct'] = (temp['Label']==temp['pred'])\n",
    "\n",
    "        #Proportion of correctly classified strides\n",
    "        proportion_strides_correct = temp.groupby('PID').aggregate({'correct': 'mean'})  \n",
    "\n",
    "        proportion_strides_correct['True Label'] = temp[['PID', 'Label']].groupby('PID').first() \n",
    "\n",
    "        #Label for the person - 0=healthy, 1=MS patient\n",
    "        proportion_strides_correct['Predicted Label'] = proportion_strides_correct['True Label']*\\\n",
    "        (proportion_strides_correct['correct']>0.5)+(1-proportion_strides_correct['True Label'])*\\\n",
    "        (proportion_strides_correct['correct']<0.5) \n",
    "\n",
    "        #Probability of class 1 - MS patient for AUC calculation\n",
    "        proportion_strides_correct['prob_class1'] = (1-proportion_strides_correct['True Label'])*\\\n",
    "        (1-proportion_strides_correct['correct'])+ proportion_strides_correct['True Label']*proportion_strides_correct['correct'] \n",
    "\n",
    "        fpr, tpr, _ = roc_curve(proportion_strides_correct['True Label'], proportion_strides_correct['prob_class1'])\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tpr_list.append(tpr)\n",
    "\n",
    "        #Person wise metrics for each fold \n",
    "        person_acc.append(accuracy_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_p.append(precision_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_r.append(recall_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_f1.append(f1_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_auc.append(roc_auc_score(proportion_strides_correct['True Label'], proportion_strides_correct['prob_class1']))\n",
    "\n",
    "    #Mean and standard deviation for person-based metrics \n",
    "    person_means = [np.mean(person_acc), np.mean(person_p), np.mean(person_r), np.mean(person_f1), np.mean(person_auc)]\n",
    "    person_stds = [np.std(person_acc), np.std(person_p), np.std(person_r), np.std(person_f1), np.std(person_auc)]\n",
    "    print('Person-based model performance (mean): ', person_means)\n",
    "    print('Person-based model performance (standard deviation): ', person_stds)\n",
    "\n",
    "    return tpr_list, [stride_metrics_mean, stride_metrics_std, person_means, person_stds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true,y_pred):\n",
    "    global yoriginal, ypredicted\n",
    "    yoriginal.append(y_true)\n",
    "    ypredicted.append(y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do not use LDA/QDA since our features are not normally distributed \n",
    "def models(X, Y, model_name = 'random_forest'):\n",
    "    '''\n",
    "    X, Y, PID groups so that strides of each person are either in training or in testing set\n",
    "    model: model_name\n",
    "    '''\n",
    "    Y_ = Y['Label'] #Dropping the PID\n",
    "    groups_ = Y['PID']\n",
    "    gkf = GroupKFold(n_splits=5) \n",
    "    scores={'accuracy': make_scorer(acc), 'precision':'precision', 'recall':'recall', 'f1': 'f1', 'auc': 'roc_auc'}\n",
    "    \n",
    "    if(model_name == 'random_forest'): #Random Forest\n",
    "        grid = {\n",
    "       'randomforestclassifier__n_estimators': [40,45,50],\\\n",
    "       'randomforestclassifier__max_depth' : [15,20,25,None],\\\n",
    "       'randomforestclassifier__class_weight': [None, 'balanced'],\\\n",
    "       'randomforestclassifier__max_features': ['auto','sqrt','log2', None],\\\n",
    "       'randomforestclassifier__min_samples_leaf':[1,2,0.1,0.05]\n",
    "        }\n",
    "        #For z-score scaling on training and use calculated coefficients on test set\n",
    "        rf_grid = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "        grid_search = GridSearchCV(rf_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'adaboost'): #Adaboost\n",
    "        ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=0))\n",
    "        grid = {\n",
    "        'adaboostclassifier__n_estimators':[50, 75, 100, 125, 150],\\\n",
    "        'adaboostclassifier__learning_rate':[0.01,.1, 1, 1.5, 2]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(ada_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "        \n",
    "    if(model_name == 'kernel_svm'): #RBF SVM\n",
    "        svc_grid = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', probability=True, random_state=0))\n",
    "        grid = {\n",
    "        'svc__gamma':[0.0001, 0.001, 0.1, 1, 10, ]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(svc_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "\n",
    "    if(model_name == 'gbm'): #GBM\n",
    "        gbm_grid = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=0))\n",
    "        grid = {\n",
    "        'gradientboostingclassifier__learning_rate':[0.15,0.1,0.05], \\\n",
    "        'gradientboostingclassifier__n_estimators':[50, 100, 150],\\\n",
    "        'gradientboostingclassifier__max_depth':[2,4,7],\\\n",
    "        'gradientboostingclassifier__min_samples_split':[2,4], \\\n",
    "        'gradientboostingclassifier__min_samples_leaf':[1,3],\\\n",
    "        'gradientboostingclassifier__max_features':['auto','sqrt','log2', None],\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(gbm_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name=='xgboost'): #Xgboost\n",
    "        xgb_grid = make_pipeline(StandardScaler(), xgboost.XGBClassifier(random_state=0))\n",
    "        grid = {\n",
    "            'xgbclassifier__min_child_weight': [1, 5],\\\n",
    "            'xgbclassifier__gamma': [0.1, 0.5, 1, 1.5, 2],\\\n",
    "            'xgbclassifier__subsample': [0.6, 0.8, 1.0],\\\n",
    "            'xgbclassifier__colsample_bytree': [0.6, 0.8, 1.0],\\\n",
    "            'xgbclassifier__max_depth': [5, 7, 8]\n",
    "        }\n",
    "        grid_search = GridSearchCV(xgb_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'knn'): #KNN\n",
    "        knn_grid = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "        grid = {\n",
    "            'kneighborsclassifier__n_neighbors': [1, 3, 4, 5, 10],\\\n",
    "            'kneighborsclassifier__p': [1, 2, 3, 4, 5]\\\n",
    "        }\n",
    "        grid_search = GridSearchCV(knn_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "        \n",
    "    if(model_name == 'decision_tree'): #Decision Tree\n",
    "        dec_grid = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=0))\n",
    "        #For z-score scaling on training and use calculated coefficients on test set\n",
    "        grid = {'decisiontreeclassifier__min_samples_split': range(2, 50)}\n",
    "        grid_search = GridSearchCV(dec_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "\n",
    "    if(model_name == 'linear_svm'): #Linear SVM\n",
    "        lsvm_grid = make_pipeline(StandardScaler(), LinearSVC(random_state=0))\n",
    "        grid = {\n",
    "            'linearsvc__loss': ['hinge','squared_hinge'],\\\n",
    "\n",
    "        }\n",
    "        grid_search = GridSearchCV(lsvm_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    \n",
    "    if(model_name == 'logistic_regression'): #Logistic regression\n",
    "        lr_grid = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "        grid = {\n",
    "            'logisticregression__random_state': [0]}\n",
    "            \n",
    "        grid_search = GridSearchCV(lr_grid, param_grid=grid, scoring=scores\\\n",
    "                           , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)\n",
    "    grid_search.fit(X, Y_, groups=groups_) #Fitting on the training set to find the optimal hyperparameters \n",
    "    tpr_list, stride_person_metrics = evaluate(grid_search, Y, yoriginal, ypredicted)\n",
    "    return tpr_list, stride_person_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV for people generalize so no train-test split\n",
    "X_raw = raw_df.drop(['Label', 'PID', 'TrialID'], axis = 1)\n",
    "Y_raw = raw_df[['PID', 'Label']] #PID to compute person based metrics later \n",
    "\n",
    "#How to make sure test set has equal no. of MS and controls in each split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['random_forest', 'adaboost', 'kernel_svm', 'gbm', 'xgboost', 'knn', 'decision_tree',  'linear_svm', \n",
    "             'logistic_regression']\n",
    "raw_metrics = pd.DataFrame(columns = ml_models) #Dataframe to store accuracies for each ML model for raw data \n",
    "#For storing predicted probabilities for person (for class 1) to show ROC curves \n",
    "tprs_raw = pd.DataFrame(columns = ml_models) \n",
    "\n",
    "for ml_model in ml_models:\n",
    "    print (ml_model)\n",
    "    yoriginal = []\n",
    "    ypredicted = []\n",
    "    tprs, stride_person_metrics = models(X_raw, Y_raw, ml_model)\n",
    "    raw_metrics[ml_model] = sum(stride_person_metrics, [])\n",
    "    tprs_raw[ml_model] = tprs\n",
    "    print ('********************************')\n",
    "\n",
    "raw_metrics.index = ['stride_mean_accuracy', 'stride_mean_precision', 'stride_mean_recall', 'stride_mean_F1', \\\n",
    "                     'stride_mean_AUC', 'stride_std_accuracy', 'stride_std_precision', 'stride_std_recall', 'stride_std_F1', \\\n",
    "                     'stride_std_AUC','person_mean_accuracy', 'person_mean_precision', 'person_mean_recall', 'person_mean_F1',\\\n",
    "                     'person_mean_AUC', 'person_std_accuracy', 'person_std_precision', 'person_std_recall', 'person_std_F1',\\\n",
    "                     'person_std_AUC']  \n",
    "raw_metrics.to_csv(path+'..//person_generalize//person_generalize_results_raw_data.csv')\n",
    "tprs_raw.to_csv(path+'..//person_generalize//person_generalize_ROCresults_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size-N data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV for people generalize so no train-test split\n",
    "X_sizeN = sizeN_df.drop(['Label', 'PID', 'TrialID'], axis = 1)\n",
    "Y_sizeN = sizeN_df[['PID', 'Label']] #PID to compute person based metrics later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['random_forest', 'adaboost', 'kernel_svm', 'gbm', 'xgboost', 'knn', 'decision_tree',  'linear_svm', \n",
    "             'logistic_regression']\n",
    "sizeN_metrics = pd.DataFrame(columns = ml_models) #Dataframe to store accuracies for each ML model for raw data \n",
    "#For storing predicted probabilities for person (for class 1) to show ROC curves \n",
    "tprs_sizeN = pd.DataFrame(columns = ml_models) \n",
    "\n",
    "for ml_model in ml_models:\n",
    "    print (ml_model)\n",
    "    yoriginal = []\n",
    "    ypredicted = []\n",
    "    tprs, stride_person_metrics = models(X_sizeN, Y_sizeN, ml_model)\n",
    "    sizeN_metrics[ml_model] = sum(stride_person_metrics, [])\n",
    "    tprs_sizeN[ml_model] = tprs\n",
    "    print ('********************************')\n",
    "\n",
    "sizeN_metrics.index = ['stride_mean_accuracy', 'stride_mean_precision', 'stride_mean_recall', 'stride_mean_F1', \\\n",
    "                     'stride_mean_AUC', 'stride_std_accuracy', 'stride_std_precision', 'stride_std_recall', 'stride_std_F1', \\\n",
    "                     'stride_std_AUC','person_mean_accuracy', 'person_mean_precision', 'person_mean_recall', 'person_mean_F1',\\\n",
    "                     'person_mean_AUC', 'person_std_accuracy', 'person_std_precision', 'person_std_recall', 'person_std_F1',\\\n",
    "                     'person_std_AUC']  \n",
    "sizeN_metrics.to_csv(path+'..//person_generalize//person_generalize_results_sizeN_data.csv')\n",
    "tprs_sizeN.to_csv(path+'..//person_generalize//person_generalize_ROCresults_sizeN_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeN_metrics.to_csv(path+'..//person_generalize//person_generalize_results_sizeN_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeN_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, yoriginal_, ypredicted_):\n",
    "    prop_strides_appended = pd.DataFrame()\n",
    "    best_index = model.cv_results_['mean_test_accuracy'].argmax()\n",
    "    print('best_params: ', model.cv_results_['params'][best_index])\n",
    "\n",
    "    #Stride-wise metrics \n",
    "    stride_metrics_mean, stride_metrics_std = [], [] #Mean and SD of stride based metrics - Acc, P, R, F1, AUC (in order)\n",
    "    scores={'accuracy': make_scorer(acc), 'precision':'precision', 'recall':'recall', 'f1': 'f1', 'auc': 'roc_auc'}\n",
    "    for score in scores:\n",
    "        stride_metrics_mean.append(model.cv_results_['mean_test_'+score][best_index])\n",
    "        stride_metrics_std.append(model.cv_results_['std_test_'+score][best_index])\n",
    "    print('Stride-based model performance (mean): ', stride_metrics_mean)\n",
    "    print('Stride-based model performance (standard deviation): ', stride_metrics_std)\n",
    "    n_folds = 7\n",
    "    person_acc, person_p, person_r, person_f1, person_auc = [], [], [], [], []\n",
    "    #For ROC curves \n",
    "    tpr_list = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        #For each fold, there are 2 splits: test and train (in order) and we need to retrieve the index \n",
    "        #of only test set for required 5 folds (best index)\n",
    "        temp = test_features.loc[yoriginal_[(best_index*2*n_folds) + (2*i)].index] #True labels for the test strides in each fold\n",
    "        temp['pred'] = ypredicted_[(best_index*2*n_folds) + (2*i)] #Predicted labels for the strides in the test set in each fold\n",
    "\n",
    "        #Correctly classified strides i.e. 1 if stride is correctly classified and 0 if otherwise\n",
    "        temp['correct'] = (temp['Label']==temp['pred'])\n",
    "        \n",
    "        print (temp)\n",
    "        #Proportion of correctly classified strides\n",
    "        proportion_strides_correct = temp.groupby('PID').aggregate({'correct': 'mean'})  \n",
    "\n",
    "        proportion_strides_correct['True Label'] = temp[['PID', 'Label']].groupby('PID').first() \n",
    "        \n",
    "        print ('Proportion correct', proportion_strides_correct)\n",
    "        prop_strides_appended= prop_strides_appended.append(proportion_strides_correct)\n",
    "        print ('append', prop_strides_appended)\n",
    "        #Label for the person - 0=healthy, 1=MS patient\n",
    "        proportion_strides_correct['Predicted Label'] = proportion_strides_correct['True Label']*\\\n",
    "        (proportion_strides_correct['correct']>=0.495)+(1-proportion_strides_correct['True Label'])*\\\n",
    "        (proportion_strides_correct['correct']<0.495) \n",
    "\n",
    "        #Probability of class 1 - MS patient for AUC calculation\n",
    "        proportion_strides_correct['prob_class1'] = (1-proportion_strides_correct['True Label'])*\\\n",
    "        (1-proportion_strides_correct['correct'])+ proportion_strides_correct['True Label']*proportion_strides_correct['correct'] \n",
    "\n",
    "        fpr, tpr, _ = roc_curve(proportion_strides_correct['True Label'], proportion_strides_correct['prob_class1'])\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tpr_list.append(tpr)\n",
    "\n",
    "        #Person wise metrics for each fold \n",
    "        person_acc.append(accuracy_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_p.append(precision_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_r.append(recall_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_f1.append(f1_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "        person_auc.append(roc_auc_score(proportion_strides_correct['True Label'], proportion_strides_correct['prob_class1']))\n",
    "\n",
    "    #Mean and standard deviation for person-based metrics \n",
    "    person_means = [np.mean(person_acc), np.mean(person_p), np.mean(person_r), np.mean(person_f1), np.mean(person_auc)]\n",
    "    person_stds = [np.std(person_acc), np.std(person_p), np.std(person_r), np.std(person_f1), np.std(person_auc)]\n",
    "    print('Person-based model performance (mean): ', person_means)\n",
    "    print('Person-based model performance (standard deviation): ', person_stds)\n",
    "    prop_strides_appended.to_csv(path+'..//person_generalize//prop_strides_appeneded_adaboost.csv')\n",
    "    return tpr_list, [stride_metrics_mean, stride_metrics_std, person_means, person_stds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We do not use LDA/QDA since our features are not normally distributed        \n",
    "#     mlp_grid = make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(50, 100, 100, 50, 30, 10, 5), activation='relu', solver='adam' , \\\n",
    "#                                                             learning_rate = 'adaptive', learning_rate_init=0.001, \\\n",
    "#                                                              shuffle=False, max_iter = 200))\n",
    "#     #(50, 100, 100, 50, 30, 10, 5)\n",
    "#     grid = {\n",
    "#     'mlpclassifier__random_state': [0], \n",
    "#     }\n",
    "#     grid_search = GridSearchCV(mlp_grid, param_grid=grid, scoring=scores\\\n",
    "#                    , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)  #0.79 good accuracy and F1 \n",
    "    \n",
    "#     ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=0))\n",
    "#     grid = {\n",
    "#     'adaboostclassifier__base_estimator': [DecisionTreeClassifier(random_state=0, min_samples_split=17)], \n",
    "#     'adaboostclassifier__n_estimators':[120],#, 50, 75, 100, 125, 150],\\\n",
    "#     'adaboostclassifier__learning_rate':[0.8] #[0.1, 0.3, 0.5, 0.6, 0.7, ]\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(ada_grid, param_grid=grid, scoring=scores\\\n",
    "#                    , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False)  #0.79\n",
    "      \n",
    "#     ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=0))\n",
    "#     grid = {\n",
    "#     'adaboostclassifier__base_estimator': [GradientBoostingClassifier(random_state=0, learning_rate = 0.8, n_estimators = 120)], \n",
    "#     'adaboostclassifier__n_estimators':[120],#, 50, 75, 100, 125, 150],\\\n",
    "#     'adaboostclassifier__learning_rate':[0.8] #[0.1, 0.3, 0.5, 0.6, 0.7, ]\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(ada_grid, param_grid=grid, scoring=scores\\\n",
    "#                    , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False) #0.81\n",
    "\n",
    "#     ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=0))\n",
    "#     grid = {\n",
    "#     'adaboostclassifier__base_estimator': [RandomForestClassifier(random_state=0)], \n",
    "#     'adaboostclassifier__n_estimators':[120],#, 50, 75, 100, 125, 150],\\\n",
    "#     'adaboostclassifier__learning_rate':[0.8] #[0.1, 0.3, 0.5, 0.6, 0.7, ]\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(ada_grid, param_grid=grid, scoring=scores\\\n",
    "#                    , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False) #0.79\n",
    "    \n",
    "    \n",
    "#     gbm_grid = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=0))\n",
    "#     grid = {\n",
    "        \n",
    "#     'gradientboostingclassifier__learning_rate':[0.8], \\\n",
    "#     'gradientboostingclassifier__n_estimators':[120],\\\n",
    "#     'gradientboostingclassifier__max_depth':[3],\\\n",
    "#     'gradientboostingclassifier__min_samples_split':range(2, 5), \\\n",
    "# #     'gradientboostingclassifier__min_samples_leaf':[1,3],\\\n",
    "#     'gradientboostingclassifier__max_features':['auto','sqrt','log2', None],\\\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(gbm_grid, param_grid=grid, scoring=scores\\\n",
    "#                        , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False) #0.76\n",
    "\n",
    "#     grid = {\n",
    "#     'randomforestclassifier__n_estimators': range(2, 40),\\\n",
    "# #     'randomforestclassifier__max_depth' : range(2, 10),\\\n",
    "#     'randomforestclassifier__max_features': ['auto','sqrt','log2', None],\\\n",
    "# #     'randomforestclassifier__min_samples_leaf':[1,2,0.1,0.05]\n",
    "#     }\n",
    "#     #For z-score scaling on training and use calculated coefficients on test set\n",
    "#     rf_grid = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "#     grid_search = GridSearchCV(rf_grid, param_grid=grid, scoring=scores\\\n",
    "#                        , n_jobs = 1, cv=gkf.split(X, Y_, groups=groups_), refit=False) #0.74 \n",
    "    \n",
    "#     grid_search.fit(X, Y_, groups=groups_) #Fitting on the training set to find the optimal hyperparameters \n",
    "#     tpr_list, stride_person_metrics = evaluate(grid_search, Y, yoriginal, ypredicted)\n",
    "#     return tpr_list, stride_person_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true,y_pred):\n",
    "    global yoriginal, ypredicted   \n",
    "    yoriginal.append(y_true)\n",
    "    ypredicted.append(y_pred)\n",
    "#     print ('In acc: ', yoriginal, ypredicted)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_mlp(test_features, yoriginal_, ypredicted_):\n",
    "#     print (len(yoriginal_), len(ypredicted_))\n",
    "    prop_strides_appended = pd.DataFrame()\n",
    "    n_folds = 7\n",
    "    person_acc= []\n",
    "    for i in range(n_folds):\n",
    "        #For each fold, there are 2 splits: test and train (in order) and we need to retrieve the index \n",
    "        #of only test set for required 5 folds (best index)\n",
    "#         print ('index', yoriginal_[i].index)\n",
    "        temp = test_features.loc[yoriginal_[i].index] #True labels for the test strides in each fold\n",
    "        temp['pred'] = ypredicted_[i] #Predicted labels for the strides in the test set in each fold\n",
    "\n",
    "        #Correctly classified strides i.e. 1 if stride is correctly classified and 0 if otherwise\n",
    "        temp['correct'] = (temp['Label']==temp['pred'])\n",
    "        \n",
    "#         print (temp)\n",
    "        #Proportion of correctly classified strides\n",
    "        proportion_strides_correct = temp.groupby('PID').aggregate({'correct': 'mean'})  \n",
    "\n",
    "        proportion_strides_correct['True Label'] = temp[['PID', 'Label']].groupby('PID').first() \n",
    "        \n",
    "#         print ('Proportion correct', proportion_strides_correct)\n",
    "        prop_strides_appended= prop_strides_appended.append(proportion_strides_correct)\n",
    "#         print ('append', prop_strides_appended)\n",
    "        #Label for the person - 0=healthy, 1=MS patient\n",
    "        proportion_strides_correct['Predicted Label'] = proportion_strides_correct['True Label']*\\\n",
    "        (proportion_strides_correct['correct']>=0.495)+(1-proportion_strides_correct['True Label'])*\\\n",
    "        (proportion_strides_correct['correct']<0.495) \n",
    "\n",
    "        #Probability of class 1 - MS patient for AUC calculation\n",
    "        proportion_strides_correct['prob_class1'] = (1-proportion_strides_correct['True Label'])*\\\n",
    "        (1-proportion_strides_correct['correct'])+ proportion_strides_correct['True Label']*proportion_strides_correct['correct'] \n",
    "\n",
    "        #Person wise metrics for each fold \n",
    "        person_acc.append(accuracy_score(proportion_strides_correct['Predicted Label'], proportion_strides_correct['True Label']))\n",
    "    #Mean and standard deviation for person-based metrics \n",
    "    print ('Person results', np.mean(person_acc), np.std(person_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_regressN_investigate = regressN_df.drop(['Label', 'PID', 'TrialID'], axis = 1)\n",
    "Y_regressN_investigate = regressN_df[['PID', 'Label']] #PID to compute person based metrics later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['MLP']\n",
    "start = time.time()\n",
    "print (ml_models)\n",
    "X = X_regressN_investigate\n",
    "Y = Y_regressN_investigate\n",
    "Y_ = Y['Label'] #Dropping the PID\n",
    "groups_ = Y['PID']\n",
    "gkf = GroupKFold(n_splits=7) \n",
    "#     scores={'accuracy': make_scorer(acc), 'precision':'precision', 'recall':'recall', 'f1': 'f1', 'auc': 'roc_auc'}\n",
    "\n",
    "grid = {\n",
    "'random_state': [0], \n",
    "'hidden_layer_sizes': [x for x in itertools.product((21, 42, 84, 7, 5, 120),repeat=7)]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(acc)\n",
    "\n",
    "sampler = ParameterGrid(grid)\n",
    "for params in sampler:\n",
    "    yoriginal=[]\n",
    "    ypredicted=[]\n",
    "    mlp_grid = make_pipeline(StandardScaler(), MLPClassifier(activation='relu', solver='adam',\\\n",
    "                                                       learning_rate = 'adaptive', learning_rate_init=0.001, \n",
    "                                                        shuffle=False, max_iter = 200, **params))\n",
    "    scores = []\n",
    "    for ix_train, ix_test in gkf.split(X, Y_, groups=groups_): \n",
    "        clf_fitted = mlp_grid.fit(X.iloc[ix_train], Y_.iloc[ix_train])\n",
    "        score = scorer(clf_fitted, X.iloc[ix_test], Y_.iloc[ix_test])\n",
    "\n",
    "    #             print ('At this fold: ', score)\n",
    "        scores.append(score)\n",
    "    print ('params = ', params)\n",
    "    print ('Stride accuracy = ', np.mean(scores), np.std(scores))\n",
    "#     print (len(yoriginal), len(ypredicted))\n",
    "    evaluate_mlp(Y, yoriginal, ypredicted)\n",
    "\n",
    "print ('********************************')\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The max I have is 0.71 right now. So any person accuracy > 0.71 is better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regress-N data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV for people generalize so no train-test split\n",
    "X_regressN = regressN_df.drop(['Label', 'PID', 'TrialID'], axis = 1)\n",
    "Y_regressN = regressN_df[['PID', 'Label']] #PID to compute person based metrics later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_models = ['random_forest', 'adaboost', 'kernel_svm', 'gbm', 'xgboost', 'knn', 'decision_tree',  'linear_svm', \n",
    "             'logistic_regression']\n",
    "regressN_metrics = pd.DataFrame(columns = ml_models) #Dataframe to store accuracies for each ML model for raw data \n",
    "#For storing predicted probabilities for person (for class 1) to show ROC curves \n",
    "tprs_regressN = pd.DataFrame(columns = ml_models) \n",
    "\n",
    "for ml_model in ml_models:\n",
    "    print (ml_model)\n",
    "    yoriginal = []\n",
    "    ypredicted = []\n",
    "    tprs, stride_person_metrics = models(X_regressN, Y_regressN, ml_model)\n",
    "    regressN_metrics[ml_model] = sum(stride_person_metrics, [])\n",
    "    tprs_regressN[ml_model] = tprs\n",
    "    print ('********************************')\n",
    "\n",
    "regressN_metrics.index = ['stride_mean_accuracy', 'stride_mean_precision', 'stride_mean_recall', 'stride_mean_F1', \\\n",
    "                     'stride_mean_AUC', 'stride_std_accuracy', 'stride_std_precision', 'stride_std_recall', 'stride_std_F1', \\\n",
    "                     'stride_std_AUC','person_mean_accuracy', 'person_mean_precision', 'person_mean_recall', 'person_mean_F1',\\\n",
    "                     'person_mean_AUC', 'person_std_accuracy', 'person_std_precision', 'person_std_recall', 'person_std_F1',\\\n",
    "                     'person_std_AUC']  \n",
    "regressN_metrics.to_csv(path+'..//person_generalize//person_generalize_results_regressN_data.csv')\n",
    "tprs_regressN.to_csv(path+'..//person_generalize//person_generalize_ROCresults_regressN_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressN_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fpr = np.linspace(0, 1, 101)\n",
    "ml_models = ['random_forest', 'adaboost', 'kernel_svm', 'gbm', 'xgboost', 'decision_tree',  'linear_svm', \n",
    "             'logistic_regression'] #'knn'\n",
    "ml_model_names = {'random_forest': 'RF', 'adaboost': 'Adaboost', 'kernel_svm': 'RBF SVM', 'gbm': 'GBM', \\\n",
    "                  'xgboost': 'Xgboost', 'knn': 'KNN', 'decision_tree': 'DT',  'linear_svm': 'LSVM', \n",
    "             'logistic_regression': 'LR'}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, sharex=True, sharey = True, figsize=(5, 9))\n",
    "sns.despine(offset=0)\n",
    "\n",
    "# #Raw Data \n",
    "# axes[0].plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 2, color = 'k')\n",
    "# for ml_model in ml_models:\n",
    "#     tprs = tprs_raw[ml_model] # person-based prediction probabilities\n",
    "#     tprs = np.array(tprs)\n",
    "#     mean_tprs = tprs.mean(axis=0)\n",
    "#     std = tprs.std(axis=0)\n",
    "\n",
    "#     tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "#     tprs_lower = mean_tprs - std\n",
    "#     axes[0].fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "#     axes[0].plot(base_fpr, mean_tprs, label=ml_model_names[ml_model]+' (AUC = '+ str(round(raw_metrics.loc['person_mean_AUC']\n",
    "#                      [ml_model], 2)) + r'$\\pm$' + str(round(raw_metrics.loc['person_std_AUC']\n",
    "#                      [ml_model], 2)) + ')', linewidth = 2)\n",
    "# axes[0].set_ylabel('True Positive Rate')\n",
    "# axes[0].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "# axes[0].set_title('Raw data')\n",
    "\n",
    "#SizeN Data \n",
    "axes[1].plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 2, color = 'k')\n",
    "for ml_model in ml_models:\n",
    "    tprs = tprs_sizeN[ml_model] # person-based prediction probabilities\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "#     axes[1].fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "    axes[1].plot(base_fpr, mean_tprs, label=ml_model_names[ml_model]+' (AUC = '+ str(round(sizeN_metrics.loc['person_mean_AUC']\n",
    "                     [ml_model], 2)) + r'$\\pm$' + str(round(sizeN_metrics.loc['person_std_AUC']\n",
    "                     [ml_model], 2)) + ')', linewidth = 2)\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('SizeN data')\n",
    "\n",
    "#RegressN Data \n",
    "axes[2].plot([0, 1], [0, 1], linestyle='--', label='Majority (AUC = 0.5)', linewidth = 2, color = 'k')\n",
    "for ml_model in ml_models:\n",
    "    tprs = tprs_regressN[ml_model] # person-based prediction probabilities\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "#     axes[2].fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "    axes[2].plot(base_fpr, mean_tprs, label=ml_model_names[ml_model]+' (AUC = '+ str(round(regressN_metrics.loc['person_mean_AUC']\n",
    "                     [ml_model], 2)) + r'$\\pm$' + str(round(regressN_metrics.loc['person_std_AUC']\n",
    "                     [ml_model], 2)) + ')', linewidth = 2)\n",
    "axes[2].set_ylabel('True Positive Rate')\n",
    "axes[2].set_title('RegressN data')\n",
    "axes[2].legend(loc='upper center', bbox_to_anchor=(1.27, 1), ncol=1)\n",
    "\n",
    "axes[2].set_xlabel('False Positive Rate')\n",
    "plt.savefig(path + '..//person_generalize//ROC_person_generalize.png', dpi = 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
